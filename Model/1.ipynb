{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-03T09:37:25.807977Z",
     "start_time": "2025-06-03T09:37:05.734573Z"
    }
   },
   "source": [
    "from CogVideoX_ti2v import *\n",
    "\n",
    "embed_dim = 1920\n",
    "text_embed_dim = 4096\n",
    "max_text_seq_length = 226\n",
    "f, c, w, h = 49, 16, 90, 60\n",
    "latent_f = (f - 1) // 4 + 1\n",
    "time_embed_dim = 512\n",
    "\n",
    "video = torch.randn(2, latent_f, c, h, w)\n",
    "text_embedding = torch.randn(2, max_text_seq_length, text_embed_dim)\n",
    "img_embedding = torch.randn(2, max_text_seq_length, text_embed_dim)\n",
    "timestep = torch.randn(2, )\n",
    "attention_head_dim = 64\n",
    "image_rotary_emb = [torch.randn(latent_f * w * h // 4, attention_head_dim), torch.randn(latent_f * w * h // 4, attention_head_dim)]\n",
    "# cog_patch_i2v = CogVideoXPatchEmbed_TI2V(2, in_channels= 16, embed_dim=embed_dim, text_embed_dim=text_embed_dim,\n",
    "#         sample_width=w, sample_height=h, sample_frames=f, temporal_compression_ratio=4, max_text_seq_length=max_text_seq_length, use_positional_embeddings=True,\n",
    "#         image_embedding_dim=text_embed_dim, max_image_seq_length=max_text_seq_length)\n",
    "# embeds = cog_patch_i2v(text_embedding, video, image_embeds=img_embedding)\n",
    "# print(embeds.shape)\n",
    "\n",
    "cogvideo_ti2v = CogVideoXTransformer3D_TI2V(\n",
    "    num_attention_heads=30, attention_head_dim=64,\n",
    "    in_channels=c, sample_width=w, sample_height=h, sample_frames=f, patch_size=2, temporal_compression_ratio=4,\n",
    "    text_embed_dim=text_embed_dim, max_text_seq_length=max_text_seq_length,\n",
    "    image_embedding_dim=text_embed_dim, max_image_seq_length=max_text_seq_length,\n",
    "    time_embed_dim=time_embed_dim,\n",
    "    num_layers=2, use_rotary_positional_embeddings=True, use_learned_positional_embeddings=True, use_decoupled_module=True)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T09:36:39.697616700Z",
     "start_time": "2025-06-03T09:33:20.721815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pred = cogvideo_ti2v(hidden_states=video, encoder_hidden_states=text_embedding, timestep=timestep,\n",
    "    encoder_hidden_states_image=img_embedding, image_rotary_emb=image_rotary_emb)\n",
    "print(pred.sample.shape)"
   ],
   "id": "af07a6d741201ca8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T09:39:41.888001Z",
     "start_time": "2025-06-03T09:39:41.867071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "train_layer_name_lst = []\n",
    "# print(cogvideo_ti2v.state_dict().keys())\n",
    "attn_train_lst = ['to_q', 'to_k', 'to_v']       #注意力层可为调: ['to_q', 'to_k', 'to_v', 'norm_q','norm_k','norm_v','to_out']\n",
    "# for layer_name in cogvideo_ti2v.state_dict().keys():\n",
    "#     print(layer_name)\n",
    "for layer_name, states_params in cogvideo_ti2v.state_dict().items():\n",
    "    if layer_name.startswith('patch_embed.image_proj'):\n",
    "        train_layer_name_lst.append(layer_name)\n",
    "    if layer_name.startswith('decouple_module'):\n",
    "        train_layer_name_lst.append(layer_name)\n",
    "    if layer_name.startswith('transformer_blocks'):\n",
    "        if re.search(r'transformer_blocks\\.\\d+\\.norm1\\.linear2', layer_name):\n",
    "            train_layer_name_lst.append(layer_name)\n",
    "        elif re.search(r'transformer_blocks\\.\\d+\\.norm2\\.linear2', layer_name):\n",
    "            train_layer_name_lst.append(layer_name)\n",
    "        elif re.search(r'transformer_blocks\\.\\d+\\.attn1\\.', layer_name):  # 注意力层\n",
    "            for attn in attn_train_lst:\n",
    "                if attn in layer_name:\n",
    "                    train_layer_name_lst.append(layer_name)\n",
    "                    break\n",
    "print(train_layer_name_lst)"
   ],
   "id": "d4f94753ea0305c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patch_embed.image_proj.weight', 'patch_embed.image_proj.bias', 'transformer_blocks.0.norm1.linear2.weight', 'transformer_blocks.0.norm1.linear2.bias', 'transformer_blocks.0.attn1.to_q.weight', 'transformer_blocks.0.attn1.to_q.bias', 'transformer_blocks.0.attn1.to_k.weight', 'transformer_blocks.0.attn1.to_k.bias', 'transformer_blocks.0.attn1.to_v.weight', 'transformer_blocks.0.attn1.to_v.bias', 'transformer_blocks.0.norm2.linear2.weight', 'transformer_blocks.0.norm2.linear2.bias', 'transformer_blocks.1.norm1.linear2.weight', 'transformer_blocks.1.norm1.linear2.bias', 'transformer_blocks.1.attn1.to_q.weight', 'transformer_blocks.1.attn1.to_q.bias', 'transformer_blocks.1.attn1.to_k.weight', 'transformer_blocks.1.attn1.to_k.bias', 'transformer_blocks.1.attn1.to_v.weight', 'transformer_blocks.1.attn1.to_v.bias', 'transformer_blocks.1.norm2.linear2.weight', 'transformer_blocks.1.norm2.linear2.bias', 'decouple_module.norm1.weight', 'decouple_module.norm1.bias', 'decouple_module.norm2.weight', 'decouple_module.norm2.bias', 'decouple_module.output_norm.weight', 'decouple_module.output_norm.bias', 'decouple_module.text_proj.weight', 'decouple_module.text_proj.bias', 'decouple_module.attn_proj.weight', 'decouple_module.attn_proj.bias', 'decouple_module.cross_attn.in_proj_weight', 'decouple_module.cross_attn.in_proj_bias', 'decouple_module.cross_attn.out_proj.weight', 'decouple_module.cross_attn.out_proj.bias', 'decouple_module.final_proj.weight', 'decouple_module.final_proj.bias']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T10:16:30.028907Z",
     "start_time": "2025-06-22T10:16:15.199191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vit\n",
    "import torch\n",
    "import torch.nn as nn  # 网络\n",
    "import torch.nn.functional as F  # 激活函数\n",
    "import torchvision.datasets  # 样本\n",
    "from torch.utils import *  # 图案管理\n",
    "from torch.utils import data  # 预处理数据集\n",
    "from torchvision import transforms  # 工具\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # 进度条\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "\n",
    "\n",
    "embeddings = torch.randn(1, 10, 1280)\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(r'E:\\PythonLearn\\work\\SSH_Connect\\Autodl\\under2postgraudate\\Video-Generation-field\\Ours\\Multiple scene\\excluded_dir\\local_model\\model_dir\\CLIP-ViT-H-14-laion2B-s32B-b79K', local_files_only=True, use_safetensors=True)\n",
    "embeddings = image_encoder.vision_model.pre_layrnorm(embeddings)\n",
    "encoder_outputs = image_encoder.vision_model.encoder(\n",
    "        inputs_embeds=embeddings,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None)\n",
    "# last_hidden_state:[B,59,1280], [f_s,f_p]=transfomer_blocks([f_s,f_p])\n",
    "last_hidden_state = encoder_outputs[0]\n",
    "# 取出style embedding f_s [B,3,1280]\n",
    "pooled_output = last_hidden_state[:, :3, :]\n",
    "# 层归一化  pooled_output:[B,3,1280]\n",
    "pooled_output = image_encoder.vision_model.post_layernorm(pooled_output)\n",
    "out = image_encoder.visual_projection(pooled_output)\n"
   ],
   "id": "7665253db6ac33b1",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T10:19:17.957035Z",
     "start_time": "2025-06-22T10:19:17.947035Z"
    }
   },
   "cell_type": "code",
   "source": "print(out.shape)",
   "id": "789f4611ed1dc9b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7a22783627229d3d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
